{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Required Libraries to run code\n",
    "import glob\n",
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading of Car Images Done\n",
      "Reading of Bike Images Done\n",
      "Reading of Non Vehicle Images Done\n",
      "Number of Car Images Loaded: 0\n",
      "Number of Bike Images Loaded: 0\n",
      "No of Non-Vehicle Images Loaded: 0\n"
     ]
    }
   ],
   "source": [
    "#reading image paths with glob\n",
    "car_image_arr = glob.glob('./vehicles/*.png')\n",
    "car_image_arr = np.append(car_image_arr, glob.glob('./vehicles/*.jpg'))\n",
    "\n",
    "# read images and append to list\n",
    "car_images_original=[]\n",
    "for imagePath in car_image_arr:\n",
    "    readImage=cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    car_images_original.append(rgbImage)\n",
    "\n",
    "print('Reading of Car Images Done')\n",
    "\n",
    "bike_image_arr = glob.glob('./bike/*.jpg')\n",
    "bike_images_original = []\n",
    "for imagePath in bike_image_arr:\n",
    "    readImage = cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    bike_images_original.append(rgbImage)\n",
    "\n",
    "print('Reading of Bike Images Done')\n",
    "\n",
    "non_vehicle_image_arr = glob.glob('./non-vehicles/non-vehicles/*.png')\n",
    "non_vehicle_image_arr = np.append(non_vehicle_image_arr, glob.glob('./non-vehicles/non-vehicles/*.jpg'))\n",
    "non_vehicle_images_original=[]\n",
    "for imagePath in non_vehicle_image_arr:\n",
    "    readImage=cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    non_vehicle_images_original.append(rgbImage)\n",
    "\n",
    "print(\"Reading of Non Vehicle Images Done\")\n",
    "\n",
    "print(\"Number of Car Images Loaded: \"+ str(len(car_images_original)))\n",
    "print(\"Number of Bike Images Loaded: \" + str(len(bike_images_original)))\n",
    "print(\"No of Non-Vehicle Images Loaded: \"+ str(len(non_vehicle_images_original)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2- Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG ( Histogram of Oriented Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General method to extact the HOG of the image\n",
    "\n",
    "def GetFeaturesFromHog(image,orient,cellsPerBlock,pixelsPerCell, visualize= False, feature_vector_flag=True):\n",
    "    if(visualize==True):\n",
    "        hog_features, hog_image = hog(image, orientations=orient,\n",
    "                                      pixels_per_cell=(pixelsPerCell, pixelsPerCell), \n",
    "                                      cells_per_block=(cellsPerBlock, cellsPerBlock), \n",
    "                                      block_norm='L2',\n",
    "                                      visualize=True, feature_vector=feature_vector_flag)\n",
    "        return hog_features, hog_image\n",
    "    else:\n",
    "        hog_features = hog(image, orientations=orient,\n",
    "                           pixels_per_cell=(pixelsPerCell, pixelsPerCell), \n",
    "                           cells_per_block=(cellsPerBlock, cellsPerBlock), \n",
    "                           block_norm='L2',\n",
    "                           visualize=False, feature_vector=feature_vector_flag)\n",
    "        return hog_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3- Generate Features Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Image Color Space. Note the colorspace parameter is like cv2.COLOR_RGB2YUV\n",
    "def ConvertImageColorspace(image, colorspace):\n",
    "    return cv2.cvtColor(image, colorspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to extract the features based on the choices as available in step 2\n",
    "\n",
    "def ExtractFeatures(images,orientation,cellsPerBlock,pixelsPerCell, convertColorspace=False):\n",
    "    featureList=[]\n",
    "    imageList=[]\n",
    "    for image in images:\n",
    "        if(convertColorspace==True):\n",
    "            image= cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "        local_features_1=GetFeaturesFromHog(image[:,:,0],orientation,cellsPerBlock,pixelsPerCell, False, True)\n",
    "        local_features_2=GetFeaturesFromHog(image[:,:,1],orientation,cellsPerBlock,pixelsPerCell, False, True)\n",
    "        local_features_3=GetFeaturesFromHog(image[:,:,2],orientation,cellsPerBlock,pixelsPerCell, False, True)\n",
    "        x=np.hstack((local_features_1,local_features_2,local_features_3))\n",
    "        featureList.append(x)\n",
    "    return featureList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Features of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "orientations=9\n",
    "cellsPerBlock=2\n",
    "pixelsPerBlock=16\n",
    "convertColorSpace=True\n",
    "carFeatures= ExtractFeatures(car_images_original,\n",
    "                             orientations,\n",
    "                             cellsPerBlock,\n",
    "                             pixelsPerBlock,\n",
    "                             convertColorSpace)\n",
    "\n",
    "bikeFeatures = ExtractFeatures(bike_images_original,\n",
    "                               orientations,\n",
    "                               cellsPerBlock,\n",
    "                               pixelsPerBlock,\n",
    "                               convertColorSpace)\n",
    "\n",
    "nonVehicleFeatures= ExtractFeatures(non_vehicle_images_original,\n",
    "                                    orientations,\n",
    "                                    cellsPerBlock,\n",
    "                                    pixelsPerBlock,\n",
    "                                    convertColorSpace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features list is  (3, 0)\n",
      "Shape of label list is  (0,)\n"
     ]
    }
   ],
   "source": [
    "featuresList= np.vstack([carFeatures,bikeFeatures,nonVehicleFeatures])\n",
    "print(\"Shape of features list is \", featuresList.shape)\n",
    "labelList= np.concatenate([np.ones(len(carFeatures)), np.ones(len(bikeFeatures))*2, np.zeros(len(nonVehicleFeatures))])\n",
    "print(\"Shape of label list is \", labelList.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4- Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 - Splitting Data into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3d8b0e464611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2184\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 0]"
     ]
    }
   ],
   "source": [
    "# train test split of data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,  X_test,Y_train, Y_test = train_test_split(featuresList, labelList, test_size=0.2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 - Normalization and Scaling of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization and scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler= StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled= scaler.transform(X_train)\n",
    "X_test_scaled= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5- Define and Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train a Linear SVM classifer\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier1= LinearSVC()\n",
    "classifier1.fit(X_train,Y_train)\n",
    "print(\"Accuracy of SVC is  \", classifier1.score(X_test,Y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw sliding Windows\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        r=random.randint(0,255)\n",
    "        g=random.randint(0,255)\n",
    "        b=random.randint(0,255)\n",
    "        color=(r, g, b)\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the windows on which we are going to run the classifier\n",
    "\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.9, 0.9)):\n",
    "   \n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0]=0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1]=img.shape[1]\n",
    "    if y_start_stop[0] ==  None:\n",
    "        y_start_stop[0]= 0\n",
    "    if y_start_stop[1] ==  None:\n",
    "        y_start_stop[1]=img.shape[0]\n",
    "    \n",
    "    \n",
    "    window_list = []\n",
    "    image_width_x= x_start_stop[1] - x_start_stop[0]\n",
    "    image_width_y= y_start_stop[1] - y_start_stop[0]\n",
    "     \n",
    "    windows_x = np.int( 1 + (image_width_x - xy_window[0])/(xy_window[0] * xy_overlap[0]))\n",
    "    windows_y = np.int( 1 + (image_width_y - xy_window[1])/(xy_window[1] * xy_overlap[1]))\n",
    "    \n",
    "    modified_window_size= xy_window\n",
    "    for i in range(0,windows_y):\n",
    "        y_start = y_start_stop[0] + np.int( i * modified_window_size[1] * xy_overlap[1])\n",
    "        for j in range(0,windows_x):\n",
    "            x_start = x_start_stop[0] + np.int( j * modified_window_size[0] * xy_overlap[0])\n",
    "            \n",
    "            x1 = np.int( x_start +  modified_window_size[0])\n",
    "            y1= np.int( y_start + modified_window_size[1])\n",
    "            window_list.append(((x_start,y_start),(x1,y1)))\n",
    "    return window_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the refined Windows\n",
    "# From Refined Windows we mean that the windows where the classifier predicts the output to be a car\n",
    "\n",
    "def DrawCars(image,windows, converColorspace=False):\n",
    "    refinedWindows=[]\n",
    "    for window in windows:\n",
    "        \n",
    "        start= window[0]\n",
    "        end= window[1]\n",
    "        clippedImage=image[start[1]:end[1], start[0]:end[0]]\n",
    "\n",
    "        if(clippedImage.shape[1] == clippedImage.shape[0] and clippedImage.shape[1]!=0):\n",
    "            clippedImage=cv2.resize(clippedImage, (64,64))\n",
    "            \n",
    "            f1=ExtractFeatures([clippedImage], 9 , 2 , 16,converColorspace)\n",
    "        \n",
    "            predictedOutput=classifier1.predict([f1[0]])\n",
    "            if(predictedOutput==1 or predictedOutput==2):\n",
    "                refinedWindows.append(window)\n",
    "#             elif(predictedOutput==2):\n",
    "#                 refinedWindows.append(window)\n",
    "        \n",
    "    return refinedWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out SubSampling using HOG but not able to go through as feature size is not the same.\n",
    "\n",
    "def DrawCarsOptimised(image, image1, image2,windows, converColorspace=False):\n",
    "    refinedWindows=[]\n",
    "    for window in windows:\n",
    "        \n",
    "        start= window[0]\n",
    "        end= window[1]\n",
    "        clippedImage=image[start[1]:end[1], start[0]:end[0]]\n",
    "        clippedImage1=image1[start[1]:end[1], start[0]:end[0]]\n",
    "        clippedImage2=image2[start[1]:end[1], start[0]:end[0]]\n",
    "        \n",
    "        if(clippedImage.shape[1] == clippedImage.shape[0] and clippedImage.shape[1]!=0):\n",
    "            \n",
    "            clippedImage=cv2.resize(clippedImage, (64,64)).ravel()\n",
    "            clippedImage1=cv2.resize(clippedImage1, (64,64)).ravel()\n",
    "            clippedImage2=cv2.resize(clippedImage2, (64,64)).ravel()\n",
    "            \n",
    "            #f1=ExtractFeatures([clippedImage], 9 , 2 , 16,converColorspace)\n",
    "            f1= np.hstack((clippedImage,clippedImage1,clippedImage2))\n",
    "            f1=scaler.transform(f1.reshape(1,-1))   \n",
    "            predictedOutput=classifier1.predict([f1[0]])\n",
    "            if(predictedOutput==1 or predictedOutput==2):\n",
    "                refinedWindows.append(window)\n",
    "                \n",
    "#             elif(predictedOutput==2):\n",
    "#                 refinedWindows.append(window)\n",
    "    return refinedWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#testing our functions of slide_window and draw window. Defining here dummy windows\n",
    "x_start = 0\n",
    "x_stop = 600\n",
    "y_start = 150\n",
    "y_stop = 300\n",
    "image = mpimg.imread('F:/Untitled.png')\n",
    "\n",
    "windows1 = slide_window(image, x_start_stop=[x_start, x_stop], y_start_stop=[y_start, y_stop], \n",
    "                    xy_window=(64,64), xy_overlap=(0.15, 0.15))\n",
    "windows4 = slide_window(image, x_start_stop=[x_start, x_stop], y_start_stop=[y_start, y_stop], \n",
    "                    xy_window=(80,80), xy_overlap=(0.2, 0.2))\n",
    "windows2 = slide_window(image, x_start_stop=[x_start, x_stop], y_start_stop=[y_start, y_stop], \n",
    "                    xy_window=(96,96), xy_overlap=(0.3, 0.3))\n",
    "windows3 = slide_window(image, x_start_stop=[x_start, x_stop], y_start_stop=[y_start, y_stop], \n",
    "                    xy_window=(128,128), xy_overlap=(0.5, 0.5))\n",
    "\n",
    "\n",
    "windows =  windows1 + windows2 +  windows3 + windows4\n",
    "print(\"Total No of windows are \",len(windows))\n",
    "refinedWindows=DrawCars(image,windows, True)\n",
    "\n",
    "\n",
    "\n",
    "f,axes= plt.subplots(2,1, figsize=(30,15))\n",
    "\n",
    "window_img = draw_boxes(image, windows) \n",
    "\n",
    "axes[0].imshow(window_img)\n",
    "axes[0].set_title(\"Window Coverage\")\n",
    "\n",
    "window_img = draw_boxes(image, refinedWindows) \n",
    "\n",
    "axes[1].set_title(\"Test Image with Refined Sliding Windows\")\n",
    "axes[1].imshow(window_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Applying Heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to increase the pixel by one inside each box\n",
    "\n",
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 5\n",
    "    # Return updated heatmap\n",
    "    return heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a threshold value to the image to filter out low pixel cells\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pixels with each car number and draw the final bounding boxes\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 2)\n",
    "    # Return the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testing our heat function\n",
    "\n",
    "heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "\n",
    "heat = add_heat(heat,refinedWindows)\n",
    "\n",
    "# Apply threshold to help remove false positives\n",
    "heat = apply_threshold(heat,3)\n",
    "\n",
    "# Visualize the heatmap when displaying    \n",
    "heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "heat_image=heatmap\n",
    "\n",
    "# Find final boxes from heatmap using label function\n",
    "labels = label(heatmap)\n",
    "print(\" Number of Cars found: \",labels[1])\n",
    "draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "\n",
    "f,axes= plt.subplots(2,1, figsize=(30,15))\n",
    "axes[0].imshow(heat_image,cmap='gray')\n",
    "axes[0].set_title(\"Heat Map Image\")\n",
    "axes[1].imshow(draw_img)\n",
    "axes[1].set_title(\"Final Image after applying Heat Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Averaging Rectangles over Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class to store the refined frames found from the last 15 frames\n",
    "\n",
    "class KeepTrack():\n",
    "    def __init__(self):\n",
    "        self.refinedWindows = [] \n",
    "        \n",
    "    def AddWindows(self, refinedWindow):\n",
    "        self.refinedWiZndows.append(refinedWindow)\n",
    "        frameHistory=15\n",
    "        if len(self.refinedWindows) > frameHistory:\n",
    "            self.refinedWindows = self.refinedWindows[len(self.refinedWindows)-frameHistory:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the Parameters required for the pipeline to run\n",
    "def defineWindows(image):\n",
    "    m,n,k = np.shape(image)\n",
    "    x_start = 0\n",
    "    if n < 600:\n",
    "        x_stop = n\n",
    "    else:\n",
    "        x_stop = 600\n",
    "    if m < 150:\n",
    "        y_start = 0\n",
    "        y_stop = m\n",
    "    else:\n",
    "        y_start = 150\n",
    "        y_stop = 300\n",
    "    cv2.rectangle(image, (x_start, y_start), (y_start, y_stop), (0,255,0), 2)\n",
    "    orientation=9 # No of orientations of HOG\n",
    "    cellsPerBlock=2 # No of cells per block\n",
    "    pixelsPerCell=16 # No of pixels per cell\n",
    "    xy_window=(64, 64) # window Size\n",
    "    xy_overlap=(0.15, 0.15) # Window Overlap. Please note this is different as provided by Udacity. Overlap of 0.15 means my windows are 85% overlapping with each other\n",
    "    # x_start_stop=[0, image.shape[1]] # X Coordinates to start and stop search\n",
    "    x_start_stop=[x_start, x_stop]\n",
    "    y_start_stop=[y_start, y_stop] # Y Coordinates to start and stop search\n",
    "\n",
    "    # Window 1- Size - 64x64 , Overlap-85%\n",
    "    windows_normal = slide_window(image, x_start_stop, y_start_stop, \n",
    "                        xy_window, xy_overlap)\n",
    "\n",
    "    # Window 2- Size - 80x80 , Overlap-80%\n",
    "    xy_window_1_25= (80,80)\n",
    "    xy_window_1_25_overlap=(0.2, 0.2)    \n",
    "    windows_1_25 = slide_window(image, x_start_stop, y_start_stop, \n",
    "                        xy_window_1_25, xy_window_1_25_overlap)\n",
    "\n",
    "    # Window 3- Size - 96x96 , Overlap-70%\n",
    "    xy_window_1_5= (96,96)\n",
    "    xy_window_1_5_overlap=(0.3, 0.3)    \n",
    "    windows_1_5 = slide_window(image, x_start_stop, y_start_stop, \n",
    "                        xy_window_1_5, xy_window_1_5_overlap)\n",
    "\n",
    "    # Window 4- Size - 128x128 , Overlap-50%\n",
    "    xy_window_twice_overlap=(0.5, 0.5)    \n",
    "    xy_window_twice = (128,128)\n",
    "    windows_twice = slide_window(image, x_start_stop, y_start_stop, \n",
    "                        xy_window_twice, xy_window_twice_overlap)\n",
    "\n",
    "    # Total Windows - 470\n",
    "    windows = windows_normal +  windows_1_5  + windows_twice +windows_1_25\n",
    "#     print(\"No of Windows are \",len(windows))\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a pipeline for Video Frame Processing\n",
    "# Note here the track of last 15 frames is kept\n",
    "\n",
    "def Pipeline(image):\n",
    "#     features,hog_image=GetFeaturesFromHog(image[:,:,0],orientation,cellsPerBlock,pixelsPerCell, visualise= True, feature_vector_flag=False)\n",
    "#     features1,hog_image1=GetFeaturesFromHog(image[:,:,1],orientation,cellsPerBlock,pixelsPerCell, visualise= True, feature_vector_flag=False)\n",
    "#     features2,hog_image2=GetFeaturesFromHog(image[:,:,2],orientation,cellsPerBlock,pixelsPerCell, visualise= True, feature_vector_flag=False)\n",
    "#     refinedWindows=DrawCarsOptimised(hog_image,hog_image1,hog_image2,windows, True)\n",
    "    \n",
    "#     image=find_cars(image, 400, 528, 1,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "#     image=find_cars(image, 400, 560, 1.25,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "#     image=find_cars(image, 400, 588, 1.5,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "#     image=find_cars(image, 400, 660, 2,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "    rand= random.randint(0,1)\n",
    "    if(rand<0.4):\n",
    "        refinedWindows=keepTrack.refinedWindows[:-1]\n",
    "    else:\n",
    "        refinedWindows=DrawCars(image,windows, True)\n",
    "        if len(refinedWindows) > 0:\n",
    "            keepTrack.AddWindows(refinedWindows)\n",
    "    \n",
    "    #refinedWindows=DrawCars(image,windows, True)\n",
    "#     if len(refinedWindows) > 0:\n",
    "#         keepTrack.AddWindows(refinedWindows)\n",
    "            \n",
    "    heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    \n",
    "    for refinedWindow in keepTrack.refinedWindows:\n",
    "        heat = add_heat(heat, refinedWindow)\n",
    "    \n",
    "    \n",
    "    \n",
    "    heatmap = apply_threshold(heat, 25 + len(keepTrack.refinedWindows)//2)\n",
    "    \n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a different pipeline to process the images as we do not want to keep track of previous frames here\n",
    "\n",
    "def PipelineImage(image):\n",
    "    windows = defineWindows(image)\n",
    "    refinedWindows=DrawCars(image,windows, True)\n",
    "    heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    heat = add_heat(heat,refinedWindows)  \n",
    "    heat[heat>0] = 255\n",
    "    heatmap = heat\n",
    "    heatmap = apply_threshold(heat, 4)\n",
    "#     figure = plt.figure()\n",
    "#     plt.imshow(heat)\n",
    "#     plt.show()\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "#     returnImage = np.copy(image)\n",
    "\n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10- Testing Pipeline on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test_images= glob.glob(\"./Untitled.png\")\n",
    "# test_images= glob.glob(\"./test_images/*.jpg\")\n",
    "# f, axes= plt.subplots(8,3, figsize=(20,40))\n",
    "f, (axe1, axe2)= plt.subplots(1,2, figsize=(20,20))\n",
    "\n",
    "\n",
    "# for index,image in enumerate(test_images):\n",
    "#     image = cv2.imread(image)\n",
    "\n",
    "#     image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "#     finalPic,heatmap = PipelineImage(image)\n",
    "#     axes[index,0].imshow(image)\n",
    "#     axes[index,0].set_title(\"Original Image\")\n",
    "#     axes[index,1].imshow(heatmap,cmap='gray')\n",
    "#     axes[index,1].set_title(\"Heatmap Image\")\n",
    "#     axes[index,2].imshow(finalPic)\n",
    "#     axes[index,2].set_title(\"Final Image\")\n",
    "\n",
    "    \n",
    "\n",
    "image = cv2.imread(\"./test_images/test6.jpg\")\n",
    "# image = cv2.imread(\"F:/Untitled.png\")\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "finalPic = PipelineImage(image)\n",
    "axe1.imshow(image)\n",
    "axe1.set_title(\"Original Image\")\n",
    "axe2.imshow(finalPic)\n",
    "axe2.set_title(\"Final Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11- Testing Pipeline on Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeepTrack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8733c3cce17a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeepTrack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeepTrack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meditor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvideo_output1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'output.mp4'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvideo_input1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test1.mp4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KeepTrack' is not defined"
     ]
    }
   ],
   "source": [
    "keepTrack = KeepTrack()\n",
    "import moviepy\n",
    "from moviepy.editor import VideoFileClip\n",
    "video_output1 = 'output.mp4'\n",
    "video_input1 = VideoFileClip('test1.mp4')\n",
    "processed_video = video_input1.fl_image(PipelineImage)\n",
    "%time processed_video.write_videofile(video_output1, audio=False)\n",
    "video_input1.reader.close()\n",
    "video_input1.audio.reader.close_proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
